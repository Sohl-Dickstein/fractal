# The boundary of neural network trainability is fractal

This repository contains the code used to generate all images and videos in
- The blog post []
- The short paper []

The images, videos, and raw data files I generated using this code can be found in [this Google Drive folder](https://drive.google.com/drive/folders/1-LKmtV1-kP-VJClHUuPlzRb2WPQMq4mx).

## Abstract

Some fractals -- for instance 
those associated with
the Mandelbrot and quadratic Julia sets -- 
are computed by iterating a function, and identifying the 
boundary between hyperparameters for which the resulting series diverges or remains bounded. 
Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, 
and can be extremely sensitive to small changes in hyperparameters. 
Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. 
We find that this boundary is fractal 
over more than ten decades of scale in all tested configurations.

For more details, see the blog post or paper!
